---
layout: post
title: 增强现实中图像技术相关内容整理
category: 增强现实
tags: [AR, computer vision, 增强现实, 立体视觉]
comments: yes
---

在前期找工作的过程中，把自己研究生阶段做过的增强现实相关项目做了个总结，顺便也在网上看了不少人的博客和教程。这篇博客来总结一下增强现实中关于图像技术相关的一些内容。

首先要面对的问题就是image detection的问题（以下问题来自于bilibili上“泡泡机器人”空间中一个视频所做的总结，我觉得非常好）：

1. 面对大量图片的图像库，如何能够找到最相似的图片？
2. 图像库如何存储？
3. 如何做快速检索？
4. 如何优化，让结果更加准确？
5. 有一些图片不断加入，如何做增量更新？

最好的方法，其实是买别人的云服务，以上问题自然就解决了。对于个人研究者或者致力于研究AR的研发人员来说，最好是能够通过coding来解决上述问题。

通常我们把图像库存放于服务器端，客户端读取图像帧之后将会提取其图像特征，并且与服务器端的图像库中图像进行比对，得到最相似的一张图像。这就涉及到了图像特征的提取与图像匹配。

服务器端的图像特征我们预先抽取出来，传统的方法是用Bag of Words方法来存储特征。AR在移动端的局限性就是：如何能够实时地进行图像特征抽取并检索到最相似图片。通常的摄像头帧率为fps30，即要在33ms内处理完一帧的内容才能够做到实时。当然，这33ms内，我们不只要做图像特征提取和匹配，还要做两帧图片之间的pose估计的计算，所以，留给image detection的时间只有20ms左右。

* Image detection

图像特征有很多种，SIFT、SURF、ORB、FAST、HOG等等。

通过不断地实验，只有提取FAST特征，才能够流畅的把AR做到移动端，即使是使用速度很快的ORB特征都无法做到实时，所以这也是为什么ORB-SLAM移植到手机端运行也不是很流畅的原因。

FAST特征可以快到什么程度呢？假设一张图片上有100个特征点，那么检测这张图片的FAST特征只需要1ms。通常还会用non-maximal suppression，这样可以使得提取特征具有一定的稳定性。但是FAST特征的缺点也很明显，就是无旋转和尺度不变性。旋转不变需要通过类似SIFT特征中的特征向量描述的方式来弥补，这里不用担心特征向量的计算问题，特征向量的计算是非常快的。尺度不变性通过手动提取多尺度来解决。（PTAM系统的实现中提取了4个尺度，实验表明提取8个尺度左右效果会更好）

在进行特征描述的时候，SIFT描述子将特征描述为4x4x8=128维的float向量，这样进行比较当然会比较耗时。可以做以下改进：

1. 用RootSIFT来做，可以提升一定的匹配效果。具体内容有一篇论文可以参考；
2. 降维。3x3x8 or 2x2x8 or 4x4x6；
3. 使用二进制索引。将每一个特征点用一个int索引，论文详见[Robust feature matching in 2.3 microseconds][2]。这样匹配速度会大大提高，索引可以过滤90%以上的特征点，只需匹配剩下不到10%的点即可。

> 上面说了一大堆的废话，总结一下就是特征检测用FAST，特征描述用SIFT的方式。

* Image Matching

这里我们用比BOW更好更快的一种方法，即vocabulary Tree（ORB-SLAM中就是用的这种方法）。具体内容可以参考这篇论文：Vocabulary Trees: Hierarchical clustering for large vocabularies. Github有[开源代码][1]。用词汇树的技术来做检索，在服务器端如果有40k的图像库，那么找到最像的一张只需20ms。

> 通过上述两方面的改进，detection加上matching在10ms内可以完成。如果用ORB特征，只提取特征点就需要消耗20-30ms的时间。

* Pose Detection

姿态估计通常是以图像库中检索到的图像为基准，计算当前帧与检索图像之间的姿态变换矩阵。计算出来之后就可以用OpenGL把模型很好的绘制出来了，只需要乘以这个4*4的矩阵即可。

OpenCV中给我们提供了姿态计算的方法，有findHomography和SolvePNP两种方法，但是这两种方法比较慢。这里我们最好是能够重写姿态估计的算法，通常都会比OpenCV中的方法快一些。

* Pose Tracking

AR中的跟踪通常用模板匹配的方法来做，其实不需要很多的点。15年开源的ARtoolkit中只用了4个线程来跟踪16个点，就取得了很好的跟踪效果。

> 这里需要介绍一下ARtoolkit了。在这个开源项目中，特征检测速度很慢，用的是HOG特征，比用FAST+SIFT的方式慢几倍，但是跟踪做的比较好。

模板匹配的方法有很多，常用的有SSD、NCC、ZNCC。在PTAM中是直接用像素的灰度差值相减来做的。SSD方法不能够抗光照，而实验中发现ZNCC效果要好一些。通常在做模板匹配的时候是先预测后匹配的。

另外，在做姿态跟踪的时候结合陀螺仪或者IMU来做效果会比较好，实现也相对容易。做的好一些的如国内的watchhh，国外的Kudan。

今天就先写到这里。

[1]: https://github.com/snavely/VocabTree2

[2]: https://www.edwardrosten.com/work/taylor_2009_robust.pdf
